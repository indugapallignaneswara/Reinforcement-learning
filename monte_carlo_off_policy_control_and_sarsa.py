# -*- coding: utf-8 -*-
"""Monte Carlo Off-Policy Control and SARSA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1thru2YbxA8rCxAUlP4ZaqdpX-P7tNxt_
"""

import numpy as np
import random

GRID_SIZE = 3
NUM_EPISODES_MC = 1000
NUM_EPISODES_SARSA = 1000
DISCOUNT_FACTOR = 0.9
ALPHA_SARSA = 0.1

ACTIONS = ['up', 'down', 'left', 'right']
NUM_ACTIONS = len(ACTIONS)

REWARDS = np.zeros((GRID_SIZE, GRID_SIZE))
REWARDS[2, 2] = 1

Q_TABLE_MC = np.zeros((GRID_SIZE, GRID_SIZE, NUM_ACTIONS))
Q_TABLE_SARSA = np.zeros((GRID_SIZE, GRID_SIZE, NUM_ACTIONS))

def behavior_policy_mc():
    return random.choice(range(NUM_ACTIONS))

def target_policy_mc(state):
    return np.argmax(Q_TABLE_MC[state[0], state[1]])

def epsilon_greedy_policy(state, epsilon):
    if random.random() < epsilon:
        return random.choice(range(NUM_ACTIONS))
    else:
        return np.argmax(Q_TABLE_SARSA[state[0], state[1]])

def get_next_state(state, action):
    if action == 'up':
        return max(0, state[0] - 1), state[1]
    elif action == 'down':
        return min(GRID_SIZE - 1, state[0] + 1), state[1]
    elif action == 'left':
        return state[0], max(0, state[1] - 1)
    elif action == 'right':
        return state[0], min(GRID_SIZE - 1, state[1] + 1)

def generate_episode_mc(behavior_policy):
    episode = []
    state = (0, 0)
    while state != (GRID_SIZE - 1, GRID_SIZE - 1):
        action = behavior_policy()
        next_state = get_next_state(state, ACTIONS[action])
        reward = REWARDS[next_state[0], next_state[1]]
        episode.append((state, action, reward))
        state = next_state
    return episode

def monte_carlo_off_policy_control():
    C = np.zeros((GRID_SIZE, GRID_SIZE, NUM_ACTIONS))
    for _ in range(NUM_EPISODES_MC):
        episode = generate_episode_mc(behavior_policy_mc)
        G = 0
        W = 1
        for t in range(len(episode) - 1, -1, -1):
            state, action, reward = episode[t]
            G = DISCOUNT_FACTOR * G + reward
            C[state[0], state[1], action] += W
            Q_TABLE_MC[state[0], state[1], action] += (W / C[state[0], state[1],
                        action]) * (G - Q_TABLE_MC[state[0], state[1], action])
            if action != target_policy_mc(state):
                break
            W /= 0.25

def sarsa():
    for _ in range(NUM_EPISODES_SARSA):
        state = (0, 0)
        action = epsilon_greedy_policy(state, epsilon=0.1)
        while state != (GRID_SIZE - 1, GRID_SIZE - 1):
            next_state = get_next_state(state, ACTIONS[action])
            next_action = epsilon_greedy_policy(next_state, epsilon=0.1)
            reward = REWARDS[next_state[0], next_state[1]]
            td_target = reward + DISCOUNT_FACTOR * Q_TABLE_SARSA[next_state[0],
                                                    next_state[1], next_action]
            td_error = td_target - Q_TABLE_SARSA[state[0], state[1], action]
            Q_TABLE_SARSA[state[0], state[1], action] += ALPHA_SARSA * td_error
            state = next_state
            action = next_action

monte_carlo_off_policy_control()

sarsa()

print("Q Table for Monte Carlo Off-Policy Control:")
print(Q_TABLE_MC)
print("\nQ Table for SARSA:")
print(Q_TABLE_SARSA)



